{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUu4Zp+rN37RY8cXNTHSbL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anilkumarKanasani/LangchainSideProjects/blob/main/5_ChatGPT_with_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "CoXVbDZZJ-Dh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57TOGXHXCVTm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ffba1ea-df9b-4a39-a13a-b02b6026b432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/302.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/302.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain -q\n",
        "!pip install openai -q\n",
        "!pip install huggingface_hub -q\n",
        "!pip install tiktoken -q\n",
        "!pip install faiss-cpu -q\n",
        "!pip install environs -q\n",
        "!pip install streamlit -q\n",
        "!pip install streamlit_chat -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from environs import Env\n",
        "env = Env()\n",
        "# Read .env into os.environ\n",
        "env.read_env(\"./env\")"
      ],
      "metadata": {
        "id": "9dMfk5-VGWaW"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ChatGPT App with Summarization Option"
      ],
      "metadata": {
        "id": "pv_P75IMpZBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from streamlit_chat import message\n",
        "from langchain.llms import AzureOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.chains.conversation.memory import (ConversationBufferMemory,\n",
        "                                                  ConversationSummaryMemory,\n",
        "                                                  ConversationBufferWindowMemory\n",
        "                                                  )\n",
        "\n",
        "# Preparing the model instance\n",
        "\n",
        "llm = AzureOpenAI(\n",
        "    deployment_name=\"anil-gpt\",\n",
        "    model_name=\"gpt-35-turbo-16k\",\n",
        ")\n",
        "\n",
        "\n",
        "st.set_page_config(page_title=\"Simple ChatBot with Memory\", page_icon=\":robot:\")\n",
        "st.header(\"Hey, I'm your Chat GPT with memory\")\n",
        "\n",
        "\n",
        "if \"human_messages\" not in st.session_state:\n",
        "  st.session_state.human_messages = []\n",
        "  st.session_state.ai_messages = []\n",
        "  st.session_state.llm_chain = ConversationChain(llm=llm,\n",
        "                                verbose=True,\n",
        "                                memory=ConversationSummaryMemory(llm=llm))\n",
        "\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for i in range (len(st.session_state.human_messages)):\n",
        "  message(st.session_state.human_messages[i], is_user=True)\n",
        "  message(st.session_state.ai_messages[i])\n",
        "\n",
        "\n",
        "\n",
        "if user_input := st.chat_input(\"What is up?\"):\n",
        "  st.session_state.human_messages.append(user_input)\n",
        "\n",
        "  message(user_input, is_user=True)\n",
        "\n",
        "  ai_response = st.session_state.llm_chain.predict(input=user_input)\n",
        "  st.session_state.ai_messages.append(ai_response)\n",
        "\n",
        "  message(ai_response)\n",
        "\n",
        "  with st.sidebar:\n",
        "    st.header(\"This is the sidebar\")\n",
        "    st.write(st.session_state.llm_chain.memory.buffer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plaqq-YkUld2",
        "outputId": "8125babf-b126-4deb-b7e4-6df3c517cd7a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFszP0YvUUi3",
        "outputId": "60d0df41-35ff-4c00-b819-380bd140d6d0"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[..................] \\ fetchMetadata: sill resolveWithNewModule localtunnel@2.0\u001b[0m\u001b[K\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.145.24.188:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.42s\n",
            "your url is: https://spicy-hands-enter.loca.lt\n",
            "2023-11-07 09:41:43.210 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 534, in _run_script\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/app.py\", line 12, in <module>\n",
            "    llm = AzureOpenAI(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/load/serializable.py\", line 97, in __init__\n",
            "    super().__init__(**kwargs)\n",
            "  File \"pydantic/main.py\", line 339, in pydantic.main.BaseModel.__init__\n",
            "  File \"pydantic/main.py\", line 1102, in pydantic.main.validate_model\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py\", line 266, in validate_environment\n",
            "    values[\"client\"] = openai.Completion\n",
            "AttributeError: module 'openai' has no attribute 'Completion'\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}